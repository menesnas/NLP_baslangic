{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d7991b-7c72-4085-95c4-c982b8e7760b",
   "metadata": {},
   "source": [
    "### NLP ön işleme pratiği\n",
    "    Kullanılan teknikler :\n",
    "- postag (sıfat/zamir gibi kelime türlerini bulma)\n",
    "- stopwords (tek başına anlamsız kelimeleri kaldırmak için)\n",
    "- counter (kelime sıklığı için)\n",
    "- Lemmatization / Stemming (Kök bulmak için)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab44ba-9e50-49d6-90a5-10c0feaf2e7f",
   "metadata": {},
   "source": [
    "## Twitter iklim krizi verileri ile nlp ön işleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff59916a-4a9f-462d-9af3-4d00ce60073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf11816e-e639-44f2-85b4-a8cfc387e0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tarih</th>\n",
       "      <th>id</th>\n",
       "      <th>icerik</th>\n",
       "      <th>ad</th>\n",
       "      <th>konum</th>\n",
       "      <th>begeni</th>\n",
       "      <th>rt</th>\n",
       "      <th>takipci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-31 22:59:36</td>\n",
       "      <td>1609323416154243072</td>\n",
       "      <td>@Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...</td>\n",
       "      <td>bazenzorgeliyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31 21:12:22</td>\n",
       "      <td>1609296428232510976</td>\n",
       "      <td>@wolfintoaction Kurak geçen bir yıl var ortada...</td>\n",
       "      <td>yesiturk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-31 17:09:59</td>\n",
       "      <td>1609235429089328896</td>\n",
       "      <td>2021 ; Doğanın bize verdiklerinin yanında bizi...</td>\n",
       "      <td>guvenada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-31 17:01:48</td>\n",
       "      <td>1609233370898202880</td>\n",
       "      <td>Erzurum’un yeni yıla karsız girecek olması… ko...</td>\n",
       "      <td>kizginbirkiz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-31 15:08:54</td>\n",
       "      <td>1609204959177854976</td>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>yesillersol_izm</td>\n",
       "      <td>izmir</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tarih                   id  \\\n",
       "0 2022-12-31 22:59:36  1609323416154243072   \n",
       "1 2022-12-31 21:12:22  1609296428232510976   \n",
       "2 2022-12-31 17:09:59  1609235429089328896   \n",
       "3 2022-12-31 17:01:48  1609233370898202880   \n",
       "4 2022-12-31 15:08:54  1609204959177854976   \n",
       "\n",
       "                                              icerik               ad  konum  \\\n",
       "0  @Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...   bazenzorgeliyo    NaN   \n",
       "1  @wolfintoaction Kurak geçen bir yıl var ortada...         yesiturk    NaN   \n",
       "2  2021 ; Doğanın bize verdiklerinin yanında bizi...         guvenada    NaN   \n",
       "3  Erzurum’un yeni yıla karsız girecek olması… ko...     kizginbirkiz    NaN   \n",
       "4  [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...  yesillersol_izm  izmir   \n",
       "\n",
       "   begeni  rt  takipci  \n",
       "0       1   0      145  \n",
       "1       1   0      376  \n",
       "2       1   0     3263  \n",
       "3       7   0       30  \n",
       "4       1   1     1854  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"iklim_krizi.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49dd70cf-e2d4-4488-99f7-4fd4e6d91432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tarih          0\n",
       "id             0\n",
       "icerik         0\n",
       "ad             0\n",
       "konum      13342\n",
       "begeni         0\n",
       "rt             0\n",
       "takipci        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09591bd7-ef17-4e8f-bd74-c29afe7ca360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tarih</th>\n",
       "      <th>id</th>\n",
       "      <th>icerik</th>\n",
       "      <th>ad</th>\n",
       "      <th>begeni</th>\n",
       "      <th>rt</th>\n",
       "      <th>takipci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-31 22:59:36</td>\n",
       "      <td>1609323416154243072</td>\n",
       "      <td>@Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...</td>\n",
       "      <td>bazenzorgeliyo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31 21:12:22</td>\n",
       "      <td>1609296428232510976</td>\n",
       "      <td>@wolfintoaction Kurak geçen bir yıl var ortada...</td>\n",
       "      <td>yesiturk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-31 17:09:59</td>\n",
       "      <td>1609235429089328896</td>\n",
       "      <td>2021 ; Doğanın bize verdiklerinin yanında bizi...</td>\n",
       "      <td>guvenada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-31 17:01:48</td>\n",
       "      <td>1609233370898202880</td>\n",
       "      <td>Erzurum’un yeni yıla karsız girecek olması… ko...</td>\n",
       "      <td>kizginbirkiz</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-31 15:08:54</td>\n",
       "      <td>1609204959177854976</td>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>yesillersol_izm</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tarih                   id  \\\n",
       "0 2022-12-31 22:59:36  1609323416154243072   \n",
       "1 2022-12-31 21:12:22  1609296428232510976   \n",
       "2 2022-12-31 17:09:59  1609235429089328896   \n",
       "3 2022-12-31 17:01:48  1609233370898202880   \n",
       "4 2022-12-31 15:08:54  1609204959177854976   \n",
       "\n",
       "                                              icerik               ad  begeni  \\\n",
       "0  @Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...   bazenzorgeliyo       1   \n",
       "1  @wolfintoaction Kurak geçen bir yıl var ortada...         yesiturk       1   \n",
       "2  2021 ; Doğanın bize verdiklerinin yanında bizi...         guvenada       1   \n",
       "3  Erzurum’un yeni yıla karsız girecek olması… ko...     kizginbirkiz       7   \n",
       "4  [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...  yesillersol_izm       1   \n",
       "\n",
       "   rt  takipci  \n",
       "0   0      145  \n",
       "1   0      376  \n",
       "2   0     3263  \n",
       "3   0       30  \n",
       "4   1     1854  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=\"konum\",axis=1,inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bcf45a6-17f3-4cdb-a65a-9a00e2751701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           begeni        rt   takipci\n",
      "begeni   1.000000  0.696226  0.272222\n",
      "rt       0.696226  1.000000  0.180406\n",
      "takipci  0.272222  0.180406  1.000000\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = df[['begeni','rt', 'takipci']].corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b844c5-ae46-4839-af8f-78ee789f34c7",
   "metadata": {},
   "source": [
    "### NLP kütüphaneleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5599b7f1-b223-4254-ac5f-be3de6c4515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Uyarı mesajlarını gizleyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b44478-2be5-4a1e-94a2-a875a2e2151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\menes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\menes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\menes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\menes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# POS tag'leri WordNet formatına dönüştürmek için fonksiyon\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Varsayılan olarak isim alır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fc5b6-5eab-4cb3-a001-ebdd360dc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "metin = df[\"icerik\"].tolist()\n",
    "metin[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd91bf6-eb91-41c2-8c45-064c56fd1ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metin_list = \" \".join(metin)\n",
    "kelimeler = nltk.word_tokenize(metin_list)\n",
    "counter = Counter(kelimeler)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0970b1e3-2cf3-4a2d-8fd2-8e422b2acca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens :  ['@', 'Fapej_in_Tufegi', 'YAAAAAAAAAAAAAAAAAAAAAA', 'Neyse', 'komünist', 'olmak', 'da', 'güzel', 'şey', 'eşitlik']\n"
     ]
    }
   ],
   "source": [
    "kelimeler = nltk.word_tokenize(metin_list)\n",
    "print(\"Tokens : \",kelimeler[:10]) ## Jupyter Notebook sunucusu çok büyük verilerde çıktıyı vermiyordu. Bu yüzden ilk 10 veri gösterildi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68095170-88f2-45be-b676-90d48dccbc9b",
   "metadata": {},
   "source": [
    "### Köklendirme \n",
    "- Stemming veya lemmatization kullanılır.\n",
    "- Stemming direkt köke odaklanır\n",
    "- Lemmatization anlamlı köklere odaklanır (better olan kelimenin kökü good olması gibi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c66bbae4-db06-4a38-9a60-46b0284e33a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kok sonrasi :  ['@', 'Fapej_in_Tufegi', 'YAAAAAAAAAAAAAAAAAAAAAA', 'Neyse', 'komünist', 'olmak', 'da', 'güzel', 'şey', 'eşitlik', 'falan', 'savunuyorsun', 'iklim', 'krizi', 'olmuyor', '@', 'wolfintoaction', 'Kurak', 'geçen', 'bir', 'yıl', 'var', 'ortada', 'kuraklık', 'iyice', 'baş', 'gösterip', 'gıda', 've', 'iklim', 'krizi', 'olursa', 'beton', 've', 'diğer', 'menkul', 'yahut', 'gayrimenkullerin', 'yenilip', 'içilmediği', 'için', 'gıdadan', 'daha', 'fazla', 'önem', 'teşkil', 'etmeyecek', 'gün', 'gelir', 'çerez', 'parası', 'da', 'olur', 'makyajlı', 'beton', 'evler', '2021', ';', 'Doğanın', 'bize', 'verdiklerinin', 'yanında', 'bizim', 'onu', 'nasıl', 'hunharca', 'yok', 'ettiğimizi', 'hatırlattı', '..', 'Adına', 'ister', 'iklim', 'krizi', 'deyin', ',', 'ister', 'başka', 'birşey', '..', 'https', ':', '//t.co/2rtifFHMfY', 'Erzurum', '’', 'un', 'yeni', 'yıla', 'karsız', 'girecek', 'olması…', 'korkunç…', 'küresel', 'ısınma…', 'yok', 'olan', 'mevsimler…', 'iklim', 'krizi…', 'her', 'şeyin', 'değişmesi…', 'normal', 'hiçbir', 'şey', 'kalmaması…', '[', \"Ankara'nın\", 'iklim', 'gündemi-9', ']', 'HDP', ':', 'İklim', 'krizi', 'yeni', 'kazanç', 'kapısı', 'olamaz', '-', 'Yeşil', 'Gazete', 'https', ':', '//t.co/0NGWYjSFAr', 'iklim', 'krizi', 'yüzünden', ',', 'yıl', 'başı', 'bu', 'sene', 'yaza', 'denk', 'gelmiş', 'gibi', '#', 'Noel', \"2023'te\", 'dünyayı', 'Rusya-Ukrayna', 'krizinin', 'etkileri', 've', 'enerji', 'krizi', 'riski', 'bekliyor', ':', \"2023'te\", 'dünyayı', 'Rusya-Ukrayna', 'savaşının', 'devam', 'etmesi', ',', 'petrol', 'fiyatlarının', 'artmasıyla', 'meydana', 'gelen', 'enerji', 'krizi', ',', 'gıda', 'güvensizliği', ',', 'ekonomik', 'kriz', 've', 'iklim', 'değişikliği', 'gibi…', 'https', ':', '//t.co/rRnOcexgYS', '@', 'huseyinkanturk', 'gıda', 've', 'ilaç', 'stoklamak', 'mı', 'yoksa', 'bunların', 'hisselerini', 'almak', 'mı', 'daha', 'yararlı', 'olur', '😂😂', 'birde', 'iklim', 'krizi', 'de', 'olacakmış', '.', 'Sanat']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "kok sonrasi :  ['@', 'fapej_in_tufegi', 'yaaaaaaaaaaaaaaaaaaaaaa', 'neys', 'komünist', 'olmak', 'da', 'güzel', 'şey', 'eşitlik', 'falan', 'savunuyorsun', 'iklim', 'krizi', 'olmuyor', '@', 'wolfintoact', 'kurak', 'geçen', 'bir', 'yıl', 'var', 'ortada', 'kuraklık', 'iyic', 'baş', 'gösterip', 'gıda', 've', 'iklim', 'krizi', 'olursa', 'beton', 've', 'diğer', 'menkul', 'yahut', 'gayrimenkullerin', 'yenilip', 'içilmediği', 'için', 'gıdadan', 'daha', 'fazla', 'önem', 'teşkil', 'etmeyecek', 'gün', 'gelir', 'çerez', 'parası', 'da', 'olur', 'makyajlı', 'beton', 'evler', '2021', ';', 'doğanın', 'bize', 'verdiklerinin', 'yanında', 'bizim', 'onu', 'nasıl', 'hunharca', 'yok', 'ettiğimizi', 'hatırlattı', '..', 'adına', 'ister', 'iklim', 'krizi', 'deyin', ',', 'ister', 'başka', 'birşey', '..', 'http', ':', '//t.co/2rtiffhmfi', 'erzurum', '’', 'un', 'yeni', 'yıla', 'karsız', 'girecek', 'olması…', 'korkunç…', 'küresel', 'ısınma…', 'yok', 'olan', 'mevsimler…', 'iklim', 'krizi…', 'her', 'şeyin', 'değişmesi…', 'normal', 'hiçbir', 'şey', 'kalmaması…', '[', \"ankara'nın\", 'iklim', 'gündemi-9', ']', 'hdp', ':', 'i̇klim', 'krizi', 'yeni', 'kazanç', 'kapısı', 'olamaz', '-', 'yeşil', 'gazet', 'http', ':', '//t.co/0ngwyjsfar', 'iklim', 'krizi', 'yüzünden', ',', 'yıl', 'başı', 'bu', 'sene', 'yaza', 'denk', 'gelmiş', 'gibi', '#', 'noel', \"2023'te\", 'dünyayı', 'rusya-ukrayna', 'krizinin', 'etkileri', 've', 'enerji', 'krizi', 'riski', 'bekliyor', ':', \"2023'te\", 'dünyayı', 'rusya-ukrayna', 'savaşının', 'devam', 'etmesi', ',', 'petrol', 'fiyatlarının', 'artmasıyla', 'meydana', 'gelen', 'enerji', 'krizi', ',', 'gıda', 'güvensizliği', ',', 'ekonomik', 'kriz', 've', 'iklim', 'değişikliği', 'gibi…', 'http', ':', '//t.co/rrnocexgi', '@', 'huseyinkanturk', 'gıda', 've', 'ilaç', 'stoklamak', 'mı', 'yoksa', 'bunların', 'hisselerini', 'almak', 'mı', 'daha', 'yararlı', 'olur', '😂😂', 'bird', 'iklim', 'krizi', 'de', 'olacakmış', '.', 'sanat']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "koklendirme = [ps.stem(kok) for kok in kelimeler]\n",
    "print(\"kok sonrasi : \",kelimeler[:200])\n",
    "print(\"-\"*100)\n",
    "print(\"kok sonrasi : \",koklendirme[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61ab397f-c7ca-4ef8-b495-8aef735ceb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize Edilmiş Kelimeler: @ Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA Neyse komünist olmak da güzel şey eşitlik falan savunuyorsun iklim krizi olmuyor @ wolfintoaction Kurak geçen bir yıl var ortada kuraklık iyice baş gösterip gıda ve iklim krizi olursa beton ve diğer menkul yahut gayrimenkullerin yenilip içilmediği için gıdadan daha fazla önem teşkil etmeyecek gün gelir çerez parası da olur makyajlı beton evler 2021 ; Doğanın bize verdiklerinin yanında bizim onu nasıl hunharca yok ettiğimizi hatırlattı .. Adına ister iklim krizi deyin , ister başka birşey .. http : //t.co/2rtifFHMfY Erzurum ’ un yeni yıla karsız girecek olması… korkunç… küresel ısınma… yok olan mevsimler… iklim krizi… her şeyin değişmesi… normal hiçbir şey kalmaması… [ Ankara'nın iklim gündemi-9 ] HDP : İklim krizi yeni kazanç kapısı olamaz - Yeşil Gazete http : //t.co/0NGWYjSFAr iklim krizi yüzünden , yıl başı bu sene yaza denk gelmiş gibi # Noel 2023'te dünyayı Rusya-Ukrayna krizinin etkileri ve enerji krizi riski bekliyor : 2023'te dünyayı Rusya-Ukrayna savaşının devam etmesi , petrol fiyatlarının artmasıyla meydana gelen enerji krizi , gıda güvensizliği , ekonomik kriz ve iklim değişikliği gibi… http : //t.co/rRnOcexgYS @ huseyinkanturk gıda ve ilaç stoklamak mı yoksa bunların hisselerini almak mı daha yararlı olur 😂😂 birde iklim krizi de olacakmış . Sanat\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lema_kelimeler = [lemmatizer.lemmatize(kelime) for kelime in kelimeler]\n",
    "print(\"Lemmatize Edilmiş Kelimeler:\", \" \".join(lema_kelimeler[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee2d1f4e-554d-45c9-92c6-4887d64d1bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temizlenmiş Metin:  Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA   Neyse komünist olmak da güzel şey eşitlik falan savunuyorsun iklim krizi olmuyor  wolfintoaction Kurak geçen bir yıl var ortada kuraklık iyice baş gösterip g\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ham metin : @Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \n",
      "\n",
      "Neyse komünist olmak da güzel şey eşitlik falan savunuyorsun iklim krizi olmuyor @wolfintoaction Kurak geçen bir yıl var ortada kuraklık iyice baş gösterip g\n"
     ]
    }
   ],
   "source": [
    "temiz_metin = re.sub(r'http\\S+|www\\S+|https\\S+', '', metin_list, flags=re.MULTILINE)\n",
    "temiz_metin = re.sub(r'\\W', ' ', temiz_metin)\n",
    "print(f\"Temizlenmiş Metin:\", temiz_metin[:200])\n",
    "print(\"-\"*100)\n",
    "print(f\"Ham metin :\",metin_list[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93cc8813-758c-4dbb-82f8-e6f7cb6924eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrelenmiş Kelimeler: Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA Neyse komünist olmak güzel eşitlik falan savunuyorsun iklim krizi olmuyor wolfintoaction Kurak geçen bir yıl var ortada kuraklık iyice baş gösterip gıda iklim krizi olursa beton diğer menkul yahut gayrimenkullerin yenilip içilmediği gıdadan fazla önem teşkil etmeyecek gün gelir çerez parası olur makyajlı beton evler 2021 Doğanın bize verdiklerinin yanında bizim onu hunharca yok ettiğimizi hatırlattı Adına ister iklim krizi deyin ister başka Erzurum un yeni yıla karsız girecek olması korkunç küresel ısınma yok olan mevsimler iklim krizi şeyin değişmesi normal hiçbir kalmaması Ankara nın iklim gündemi 9 HDP İklim krizi yeni kazanç kapısı olamaz Yeşil Gazete iklim krizi yüzünden yıl başı sene yaza denk gelmiş Noel 2023 te dünyayı Rusya Ukrayna krizinin etkileri enerji krizi riski bekliyor 2023 te dünyayı Rusya Ukrayna savaşının devam etmesi petrol fiyatlarının artmasıyla meydana gelen enerji krizi gıda güvensizliği ekonomik kriz iklim değişikliği huseyinkanturk gıda ilaç stoklamak yoksa bunların hisselerini almak yararlı olur birde iklim krizi olacakmış Sanat spor iklim krizi ileri dönüşüm odağımızda farkındalığımızın arttığı gidecek yolumuz görecek yıllarımız var dediğimiz yaşama karşı heyecanımızın kaybolmadığı sağlıklı mutlu huzurlu başarılı umut dolu hoş gel 2 0 2 3 HappyNewYear HappyNewYear2023 Ankara nın iklim gündemi 9 HDP İklim krizi yeni kazanç kapısı olamaz\n",
      "Yakalanmış Stopwords  : da şey ve ve için daha da nasıl birşey her şey bu gibi ve ve gibi ve mı mı daha de daha çok ile ve ve gibi biri ve ve bu ne diye ne ile Ve ve hep ve bu çok ile ve da ve ve gibi ve ya daha çok ve ve de için neden ile en ne diye ne da yani Bu Bu Bu Bu Bu Her şey çok tüm bu bu ve her gibi için bu ve ve daha bu daha çok da ve gibi ve bazı Sanki kez bu bu ki da ve mu bu Çok de ya ya ki az da bu için en ve veya çok daha ile daha ama ve biz için Bu birkaç da şey bu Aslında için bu bu da de daha ile için ya gibi da Bu mı mı yani En çok ve en ise ve için da da diye siz de daha ve ama ve tüm ile ve en tüm en her biri en az ya da Ama defa bu da ki çok ve diye hepsi de ne defa ya yani mı en çok ya bu tüm ve bu için ne şu gibi ve için ve için ve ve\n"
     ]
    }
   ],
   "source": [
    "stop_kelimeler = set(stopwords.words('turkish'))\n",
    "\n",
    "filtrelenmis_kelimeler = [kelime for kelime in nltk.word_tokenize(temiz_metin) if kelime.lower() not in stop_kelimeler]\n",
    "stop_words = [kelime for kelime in nltk.word_tokenize(temiz_metin) if kelime.lower() in stop_kelimeler]\n",
    "print(\"Filtrelenmiş Kelimeler:\", \" \".join(filtrelenmis_kelimeler[:200]))\n",
    "print(\"Yakalanmış Stopwords  :\", \" \".join(stop_words[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "438efe20-1c74-4c61-953e-7f6ad61ca3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kucuk_harfli_metin = metin_list.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02732f9c-083f-4560-91d9-2e819ebb9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizasyon_metin = re.sub(r'[^\\w\\s]', '', kucuk_harfli_metin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e2f4848-500e-4297-b7cf-89b371c90b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(series, remove_hashtag=False, remove_mentions=False, remove_links=False, remove_numbers=False,\n",
    "                  remove_short_text=False, lowercase=False, remove_punctuation=False, remove_stopwords=False,\n",
    "                  remove_rare_words=False, rare_limit=5):    \n",
    "    if lowercase:\n",
    "        print(\"Küçük harfe çeviriliyor...\")\n",
    "        start = timer()\n",
    "        series = series.str.lower()\n",
    "        print(f\"Küçük harfe çevirme işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    if remove_hashtag:\n",
    "        print(\"Hashtagler kaldırılıyor...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'((#)[^\\s]*)\\b', '', regex=True)\n",
    "        print(f\"Kaldırma işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    if remove_mentions:\n",
    "        print(\"Mentionlar kaldırılıyor...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'((@)[^\\s]*)\\b', '', regex=True)\n",
    "        print(f\"Kaldırma işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    if remove_links:\n",
    "        print(\"Linkler kaldırılıyor...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'\\n', '', regex=True)\n",
    "        series = series.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "        print(f\"Kaldırma işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    if remove_numbers:\n",
    "        print(\"Sayılar kaldırılıyor...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r'\\d+', '', regex=True)\n",
    "        print(f\"Kaldırma işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    if remove_short_text:\n",
    "        print(\"Kısa metinler kaldırılıyor...\")\n",
    "        start = timer()\n",
    "        series = series.apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x))\n",
    "        print(f\"Kaldırma işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    if remove_punctuation:\n",
    "        print(\"Noktalama işaretleri kaldırılıyor...\")\n",
    "        start = timer()\n",
    "        series = series.str.replace(r\"((')[^\\s]*)\\b\", '', regex=True)\n",
    "        series = series.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "        print(f\"Kaldırma işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    if remove_stopwords:\n",
    "        print(\"Removing stopwords...\")\n",
    "        start = timer()\n",
    "        # Stopwordsleri veri setinden kaldırma işlemi\n",
    "        series = series.apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_kelimeler]))\n",
    "        print(f\"Stopwords are removed in {timedelta(seconds=timer() - start)}\")\n",
    "\n",
    "    if remove_rare_words:\n",
    "        print(\"Nadir kelimeler kaldırılıyor...\")\n",
    "        start = timer()\n",
    "        whole_count = pd.Series(\" \".join(series).split()).value_counts()\n",
    "        print(f\"Seride toplam {whole_count.count()} kelime var.\")\n",
    "        print(f\"%{round(whole_count[whole_count <= rare_limit].count() / whole_count.count() * 100, 2)} oranında kelime, nadir limitinden ({rare_limit}) daha az kez görünüyor.\")\n",
    "        to_remove = whole_count[whole_count <= rare_limit]\n",
    "        print(\"Nadir kelimeler kaldırılıyor...\")\n",
    "        series = series.apply(lambda x: \" \".join(x for x in x.split() if x not in to_remove))\n",
    "        print(f\"{len(to_remove)} nadir kelime kaldırıldı.\")\n",
    "        print(f\"Kaldırma işlemi {timedelta(seconds=timer() - start)} içinde tamamlandı.\")\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def tr_en_char_translate(series):\n",
    "    \"\"\"\n",
    "    Tasks\n",
    "    -----\n",
    "        Convert Turkish characters to English characters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series: pandas.Series\n",
    "        The series to be translated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    series: pandas.Series\n",
    "        The translated series.\n",
    "    \"\"\"\n",
    "    series = series.str.replace('ı', 'i')\n",
    "    series = series.str.replace('ü', 'u')\n",
    "    series = series.str.replace('ö', 'o')\n",
    "    series = series.str.replace('ğ', 'g')\n",
    "    series = series.str.replace('ş', 's')\n",
    "    series = series.str.replace('ç', 'c')\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11620a03-3ea7-40ac-83ea-98ba5b161d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Küçük harfe çeviriliyor...\n",
      "Küçük harfe çevirme işlemi 0:00:00.056382 içinde tamamlandı.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icerik</th>\n",
       "      <th>icerik_lowercased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...</td>\n",
       "      <td>@fapej_in_tufegi yaaaaaaaaaaaaaaaaaaaaaa \\n\\nn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@wolfintoaction Kurak geçen bir yıl var ortada...</td>\n",
       "      <td>@wolfintoaction kurak geçen bir yıl var ortada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021 ; Doğanın bize verdiklerinin yanında bizi...</td>\n",
       "      <td>2021 ; doğanın bize verdiklerinin yanında bizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Erzurum’un yeni yıla karsız girecek olması… ko...</td>\n",
       "      <td>erzurum’un yeni yıla karsız girecek olması… ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>[ankara'nın iklim gündemi-9] hdp: i̇klim krizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iklim krizi yüzünden, yıl başı bu sene yaza de...</td>\n",
       "      <td>iklim krizi yüzünden, yıl başı bu sene yaza de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023'te dünyayı Rusya-Ukrayna krizinin etkiler...</td>\n",
       "      <td>2023'te dünyayı rusya-ukrayna krizinin etkiler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@huseyinkanturk gıda ve ilaç stoklamak mı yoks...</td>\n",
       "      <td>@huseyinkanturk gıda ve ilaç stoklamak mı yoks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sanat,spor,iklim krizi,ileri dönüşüm odağımızd...</td>\n",
       "      <td>sanat,spor,iklim krizi,ileri dönüşüm odağımızd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>[ankara'nın iklim gündemi-9] hdp: i̇klim krizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>[ankara'nın iklim gündemi-9] hdp: i̇klim krizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023'te dünyayı Rusya-Ukrayna krizinin etkiler...</td>\n",
       "      <td>2023'te dünyayı rusya-ukrayna krizinin etkiler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022 yılında sanat dünyasına damga vuran olayl...</td>\n",
       "      <td>2022 yılında sanat dünyasına damga vuran olayl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Peki enerji krizi kıtasal bağımlılıklar yaratı...</td>\n",
       "      <td>peki enerji krizi kıtasal bağımlılıklar yaratı...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cemalettin Taşcı ile Ve Fakat'ın yeni bölümünd...</td>\n",
       "      <td>cemalettin taşcı ile ve fakat'ın yeni bölümünd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023'te \"hep birlikte\" özgürce ve mutlulukla y...</td>\n",
       "      <td>2023'te \"hep birlikte\" özgürce ve mutlulukla y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kışın ortasındaki bu kuraklık çok can sıkıcı, ...</td>\n",
       "      <td>kışın ortasındaki bu kuraklık çok can sıkıcı, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‘DİRENÇLİ İŞLETMELER’ PROJESİNDE EĞİTİMLER TAM...</td>\n",
       "      <td>‘di̇rençli̇ i̇şletmeler’ projesi̇nde eği̇ti̇ml...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rusya-Ukrayna savaşının da etkileriyle bölgese...</td>\n",
       "      <td>rusya-ukrayna savaşının da etkileriyle bölgese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Küresel ısınma, iklim krizi, saldırılar, herke...</td>\n",
       "      <td>küresel ısınma, iklim krizi, saldırılar, herke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               icerik  \\\n",
       "0   @Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...   \n",
       "1   @wolfintoaction Kurak geçen bir yıl var ortada...   \n",
       "2   2021 ; Doğanın bize verdiklerinin yanında bizi...   \n",
       "3   Erzurum’un yeni yıla karsız girecek olması… ko...   \n",
       "4   [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...   \n",
       "5   iklim krizi yüzünden, yıl başı bu sene yaza de...   \n",
       "6   2023'te dünyayı Rusya-Ukrayna krizinin etkiler...   \n",
       "7   @huseyinkanturk gıda ve ilaç stoklamak mı yoks...   \n",
       "8   Sanat,spor,iklim krizi,ileri dönüşüm odağımızd...   \n",
       "9   [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...   \n",
       "10  [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...   \n",
       "11  2023'te dünyayı Rusya-Ukrayna krizinin etkiler...   \n",
       "12  2022 yılında sanat dünyasına damga vuran olayl...   \n",
       "13  Peki enerji krizi kıtasal bağımlılıklar yaratı...   \n",
       "14  Cemalettin Taşcı ile Ve Fakat'ın yeni bölümünd...   \n",
       "15  2023'te \"hep birlikte\" özgürce ve mutlulukla y...   \n",
       "16  Kışın ortasındaki bu kuraklık çok can sıkıcı, ...   \n",
       "17  ‘DİRENÇLİ İŞLETMELER’ PROJESİNDE EĞİTİMLER TAM...   \n",
       "18  Rusya-Ukrayna savaşının da etkileriyle bölgese...   \n",
       "19  Küresel ısınma, iklim krizi, saldırılar, herke...   \n",
       "\n",
       "                                    icerik_lowercased  \n",
       "0   @fapej_in_tufegi yaaaaaaaaaaaaaaaaaaaaaa \\n\\nn...  \n",
       "1   @wolfintoaction kurak geçen bir yıl var ortada...  \n",
       "2   2021 ; doğanın bize verdiklerinin yanında bizi...  \n",
       "3   erzurum’un yeni yıla karsız girecek olması… ko...  \n",
       "4   [ankara'nın iklim gündemi-9] hdp: i̇klim krizi...  \n",
       "5   iklim krizi yüzünden, yıl başı bu sene yaza de...  \n",
       "6   2023'te dünyayı rusya-ukrayna krizinin etkiler...  \n",
       "7   @huseyinkanturk gıda ve ilaç stoklamak mı yoks...  \n",
       "8   sanat,spor,iklim krizi,ileri dönüşüm odağımızd...  \n",
       "9   [ankara'nın iklim gündemi-9] hdp: i̇klim krizi...  \n",
       "10  [ankara'nın iklim gündemi-9] hdp: i̇klim krizi...  \n",
       "11  2023'te dünyayı rusya-ukrayna krizinin etkiler...  \n",
       "12  2022 yılında sanat dünyasına damga vuran olayl...  \n",
       "13  peki enerji krizi kıtasal bağımlılıklar yaratı...  \n",
       "14  cemalettin taşcı ile ve fakat'ın yeni bölümünd...  \n",
       "15  2023'te \"hep birlikte\" özgürce ve mutlulukla y...  \n",
       "16  kışın ortasındaki bu kuraklık çok can sıkıcı, ...  \n",
       "17  ‘di̇rençli̇ i̇şletmeler’ projesi̇nde eği̇ti̇ml...  \n",
       "18  rusya-ukrayna savaşının da etkileriyle bölgese...  \n",
       "19  küresel ısınma, iklim krizi, saldırılar, herke...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['icerik_lowercased'] = preprocessing(df['icerik'], lowercase=True)\n",
    "df[['icerik', 'icerik_lowercased']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcb57b1f-124b-436a-b33e-2c7fb0fcb693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mentionlar kaldırılıyor...\n",
      "Kaldırma işlemi 0:00:00.030039 içinde tamamlandı.\n",
      "Linkler kaldırılıyor...\n",
      "Kaldırma işlemi 0:00:00.053703 içinde tamamlandı.\n",
      "Sayılar kaldırılıyor...\n",
      "Kaldırma işlemi 0:00:00.091331 içinde tamamlandı.\n",
      "Kısa metinler kaldırılıyor...\n",
      "Kaldırma işlemi 0:00:00.165192 içinde tamamlandı.\n",
      "Noktalama işaretleri kaldırılıyor...\n",
      "Kaldırma işlemi 0:00:00.114298 içinde tamamlandı.\n",
      "Nadir kelimeler kaldırılıyor...\n",
      "Seride toplam 93814 kelime var.\n",
      "%63.46 oranında kelime, nadir limitinden (1) daha az kez görünüyor.\n",
      "Nadir kelimeler kaldırılıyor...\n",
      "59530 nadir kelime kaldırıldı.\n",
      "Kaldırma işlemi 0:00:01.366426 içinde tamamlandı.\n",
      "Removing stopwords...\n",
      "Stopwords are removed in 0:00:00.130652\n"
     ]
    }
   ],
   "source": [
    "df['icerik_mentions'] = preprocessing(df['icerik_lowercased'], remove_mentions=True)\n",
    "df['icerik_links'] = preprocessing(df['icerik_mentions'], remove_links=True)\n",
    "df['icerik_numbers'] = preprocessing(df['icerik_links'], remove_numbers=True)\n",
    "df['icerik_short_text'] = preprocessing(df['icerik_numbers'], remove_short_text=True)\n",
    "df['icerik_punctuation'] = preprocessing(df['icerik_short_text'], remove_punctuation=True)\n",
    "df['icerik_rare_words'] = preprocessing(df['icerik_punctuation'], remove_rare_words=True, rare_limit=1)\n",
    "df['icerik_stopwords'] = preprocessing(df['icerik_rare_words'], remove_stopwords=True)\n",
    "df['icerik_tr_en_char'] = tr_en_char_translate(df['icerik_stopwords'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cb59f65-6490-4191-94b9-fe021ad92f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icerik</th>\n",
       "      <th>icerik_tr_en_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...</td>\n",
       "      <td>neyse komunist olmak guzel esitlik falan iklim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@wolfintoaction Kurak geçen bir yıl var ortada...</td>\n",
       "      <td>kurak gecen bir yil var ortada kuraklik iyice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021 ; Doğanın bize verdiklerinin yanında bizi...</td>\n",
       "      <td>doganin bize yaninda bizim onu hunharca yok ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Erzurum’un yeni yıla karsız girecek olması… ko...</td>\n",
       "      <td>erzurum yeni yila karsiz girecek olmasi korkun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>ankara iklim gundemi hdp klim krizi yeni kazan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iklim krizi yüzünden, yıl başı bu sene yaza de...</td>\n",
       "      <td>iklim krizi yuzunden yil basi sene yaza denk g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023'te dünyayı Rusya-Ukrayna krizinin etkiler...</td>\n",
       "      <td>dunyayi rusyaukrayna krizinin etkileri enerji ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@huseyinkanturk gıda ve ilaç stoklamak mı yoks...</td>\n",
       "      <td>gida ilac stoklamak yoksa bunlarin almak yarar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sanat,spor,iklim krizi,ileri dönüşüm odağımızd...</td>\n",
       "      <td>donusum arttigi gidecek yolumuz gorecek yillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>ankara iklim gundemi hdp klim krizi yeni kazan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Ankara'nın iklim gündemi-9] HDP: İklim krizi ...</td>\n",
       "      <td>ankara iklim gundemi hdp klim krizi yeni kazan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023'te dünyayı Rusya-Ukrayna krizinin etkiler...</td>\n",
       "      <td>dunyayi rusyaukrayna krizinin etkileri enerji ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022 yılında sanat dünyasına damga vuran olayl...</td>\n",
       "      <td>yilinda sanat dunyasina damga vuran olaylardan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Peki enerji krizi kıtasal bağımlılıklar yaratı...</td>\n",
       "      <td>peki enerji krizi kitasal bagimliliklar yarati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cemalettin Taşcı ile Ve Fakat'ın yeni bölümünd...</td>\n",
       "      <td>fakat yeni bolumunde iklim krizi iklim aktivis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023'te \"hep birlikte\" özgürce ve mutlulukla y...</td>\n",
       "      <td>birlikte ozgurce yasamak dilegiyle veganuary v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kışın ortasındaki bu kuraklık çok can sıkıcı, ...</td>\n",
       "      <td>kisin ortasindaki kuraklik can sikici iklim kr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‘DİRENÇLİ İŞLETMELER’ PROJESİNDE EĞİTİMLER TAM...</td>\n",
       "      <td>sletmeler projesinde genc nsanlari dernegi ikl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rusya-Ukrayna savaşının da etkileriyle bölgese...</td>\n",
       "      <td>rusyaukrayna savasinin etkileriyle bolgesel ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Küresel ısınma, iklim krizi, saldırılar, herke...</td>\n",
       "      <td>kuresel isinma iklim krizi saldirilar herkesin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               icerik  \\\n",
       "0   @Fapej_in_Tufegi YAAAAAAAAAAAAAAAAAAAAAA \\n\\nN...   \n",
       "1   @wolfintoaction Kurak geçen bir yıl var ortada...   \n",
       "2   2021 ; Doğanın bize verdiklerinin yanında bizi...   \n",
       "3   Erzurum’un yeni yıla karsız girecek olması… ko...   \n",
       "4   [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...   \n",
       "5   iklim krizi yüzünden, yıl başı bu sene yaza de...   \n",
       "6   2023'te dünyayı Rusya-Ukrayna krizinin etkiler...   \n",
       "7   @huseyinkanturk gıda ve ilaç stoklamak mı yoks...   \n",
       "8   Sanat,spor,iklim krizi,ileri dönüşüm odağımızd...   \n",
       "9   [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...   \n",
       "10  [Ankara'nın iklim gündemi-9] HDP: İklim krizi ...   \n",
       "11  2023'te dünyayı Rusya-Ukrayna krizinin etkiler...   \n",
       "12  2022 yılında sanat dünyasına damga vuran olayl...   \n",
       "13  Peki enerji krizi kıtasal bağımlılıklar yaratı...   \n",
       "14  Cemalettin Taşcı ile Ve Fakat'ın yeni bölümünd...   \n",
       "15  2023'te \"hep birlikte\" özgürce ve mutlulukla y...   \n",
       "16  Kışın ortasındaki bu kuraklık çok can sıkıcı, ...   \n",
       "17  ‘DİRENÇLİ İŞLETMELER’ PROJESİNDE EĞİTİMLER TAM...   \n",
       "18  Rusya-Ukrayna savaşının da etkileriyle bölgese...   \n",
       "19  Küresel ısınma, iklim krizi, saldırılar, herke...   \n",
       "\n",
       "                                    icerik_tr_en_char  \n",
       "0   neyse komunist olmak guzel esitlik falan iklim...  \n",
       "1   kurak gecen bir yil var ortada kuraklik iyice ...  \n",
       "2   doganin bize yaninda bizim onu hunharca yok ha...  \n",
       "3   erzurum yeni yila karsiz girecek olmasi korkun...  \n",
       "4   ankara iklim gundemi hdp klim krizi yeni kazan...  \n",
       "5   iklim krizi yuzunden yil basi sene yaza denk g...  \n",
       "6   dunyayi rusyaukrayna krizinin etkileri enerji ...  \n",
       "7   gida ilac stoklamak yoksa bunlarin almak yarar...  \n",
       "8   donusum arttigi gidecek yolumuz gorecek yillar...  \n",
       "9   ankara iklim gundemi hdp klim krizi yeni kazan...  \n",
       "10  ankara iklim gundemi hdp klim krizi yeni kazan...  \n",
       "11  dunyayi rusyaukrayna krizinin etkileri enerji ...  \n",
       "12  yilinda sanat dunyasina damga vuran olaylardan...  \n",
       "13  peki enerji krizi kitasal bagimliliklar yarati...  \n",
       "14  fakat yeni bolumunde iklim krizi iklim aktivis...  \n",
       "15  birlikte ozgurce yasamak dilegiyle veganuary v...  \n",
       "16  kisin ortasindaki kuraklik can sikici iklim kr...  \n",
       "17  sletmeler projesinde genc nsanlari dernegi ikl...  \n",
       "18  rusyaukrayna savasinin etkileriyle bolgesel ku...  \n",
       "19  kuresel isinma iklim krizi saldirilar herkesin...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['icerik', 'icerik_tr_en_char']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07e6642a-2b3c-4e29-a5a6-22349152ab82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sütunlardaki metinler birleştiriliyor.\n",
      "Toplam kelime sayıları hesaplanıyor.\n",
      "Farklı kelime sayıları hesaplanıyor.\n",
      "En çok tekrar eden 10 kelime hesaplanıyor.\n",
      "En az tekrar eden 10 kelime hesaplanıyor.\n",
      "Sütun 1: icerik\n",
      "Toplam kelime sayısı: 747547\n",
      "Farklı kelime sayısı: 161544\n",
      "En çok tekrar eden 10 kelime: [('iklim', 25495), ('krizi', 21082), ('ve', 15160), ('bir', 9972), ('bu', 5269), ('için', 5057), ('#iklimkrizi', 5032), ('ile', 4429), ('de', 4066), ('da', 3734)]\n",
      "En az tekrar eden 10 kelime: [('alınmamasına', 1), ('gireyim', 1), ('tutardı', 1), ('https://t.co/AgSr3bjsuO', 1), ('netliğiyle', 1), ('ilkeler.', 1), ('gazetecisi', 1), ('https://t.co/ABdKzTjvog', 1), ('sürdürülsün.', 1), ('santralleriyle', 1)]\n",
      "\n",
      "\n",
      "Sütun 2: icerik_tr_en_char\n",
      "Toplam kelime sayısı: 528373\n",
      "Farklı kelime sayısı: 32760\n",
      "En çok tekrar eden 10 kelime: [('iklim', 26677), ('krizi', 25839), ('bir', 10608), ('kuresel', 3722), ('klim', 3602), ('iklimkrizi', 3585), ('var', 2956), ('dunya', 2784), ('yok', 2496), ('degil', 2141)]\n",
      "En az tekrar eden 10 kelime: [('dinlemedik', 2), ('font', 2), ('duzeltilmesini', 2), ('yayinlayacagi', 2), ('yayinlayacakbiden', 2), ('devralmasinin', 2), ('grace', 2), ('dae', 2), ('deppoefes', 2), ('esmiyorsabirnedenivar', 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Metni temizle ve kelimelere ayır\n",
    "def preprocess_text(text):\n",
    "    words = text.split()  # Kelimelere ayır\n",
    "    return words\n",
    "\n",
    "\n",
    "def analyze_columns(df, column1, column2):\n",
    "    # Sütunlardaki metinleri birleştir ve kelimelere ayır\n",
    "    print(\"Sütunlardaki metinler birleştiriliyor.\")\n",
    "    words1 = df[column1].apply(preprocess_text).sum()\n",
    "    words2 = df[column2].apply(preprocess_text).sum()\n",
    "\n",
    "    # Toplam kelime sayıları\n",
    "    print(\"Toplam kelime sayıları hesaplanıyor.\")\n",
    "    total_words1 = len(words1)\n",
    "    total_words2 = len(words2)\n",
    "\n",
    "    # Farklı kelime sayıları\n",
    "    print(\"Farklı kelime sayıları hesaplanıyor.\")\n",
    "    unique_words1 = len(set(words1))\n",
    "    unique_words2 = len(set(words2))\n",
    "\n",
    "    # En çok tekrar eden 10 kelime\n",
    "    print(\"En çok tekrar eden 10 kelime hesaplanıyor.\")\n",
    "    common_words1 = Counter(words1).most_common(10)\n",
    "    common_words2 = Counter(words2).most_common(10)\n",
    "\n",
    "    # En az tekrar eden 10 kelime\n",
    "    print(\"En az tekrar eden 10 kelime hesaplanıyor.\")\n",
    "    least_common_words1 = Counter(words1).most_common()[:-11:-1]\n",
    "    least_common_words2 = Counter(words2).most_common()[:-11:-1]\n",
    "\n",
    "    # Sonuçları ekrana bas\n",
    "    print(f\"Sütun 1: {column1}\")\n",
    "    print(f\"Toplam kelime sayısı: {total_words1}\")\n",
    "    print(f\"Farklı kelime sayısı: {unique_words1}\")\n",
    "    print(f\"En çok tekrar eden 10 kelime: {common_words1}\")\n",
    "    print(f\"En az tekrar eden 10 kelime: {least_common_words1}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Sütun 2: {column2}\")\n",
    "    print(f\"Toplam kelime sayısı: {total_words2}\")\n",
    "    print(f\"Farklı kelime sayısı: {unique_words2}\")\n",
    "    print(f\"En çok tekrar eden 10 kelime: {common_words2}\")\n",
    "    print(f\"En az tekrar eden 10 kelime: {least_common_words2}\")\n",
    "\n",
    "\n",
    "# DataFrame'deki sütunları analiz et\n",
    "analyze_columns(df, 'icerik', 'icerik_tr_en_char')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfdaee-525f-4b33-8fc6-0d62a00596da",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e2863-77dc-486a-b94a-6ab3cb379bc0",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d2f151-d97d-46cf-a11a-fb821430311c",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d548c0-5680-4ecc-9ffa-d33e27ab74b2",
   "metadata": {},
   "source": [
    "## Twitter verileri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "991b4d8b-c10b-4679-96d0-0e4148c2b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "veriler = pd.read_csv(\"twitter_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "fccd97cf-e0de-4890-9667-74d75221dc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>julie81</td>\n",
       "      <td>Party least receive say or single. Prevent pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-30 11:00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>richardhester</td>\n",
       "      <td>Hotel still Congress may member staff. Media d...</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-02 22:45:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>williamsjoseph</td>\n",
       "      <td>Nice be her debate industry that year. Film wh...</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-18 11:25:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>danielsmary</td>\n",
       "      <td>Laugh explain situation career occur serious. ...</td>\n",
       "      <td>37</td>\n",
       "      <td>18</td>\n",
       "      <td>2023-04-10 22:06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>carlwarren</td>\n",
       "      <td>Involve sense former often approach government...</td>\n",
       "      <td>27</td>\n",
       "      <td>80</td>\n",
       "      <td>2023-01-24 07:12:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID        Username                                               Text  Retweets  Likes            Timestamp\n",
       "0         1         julie81  Party least receive say or single. Prevent pre...         2     25  2023-01-30 11:00:51\n",
       "1         2   richardhester  Hotel still Congress may member staff. Media d...        35     29  2023-01-02 22:45:58\n",
       "2         3  williamsjoseph  Nice be her debate industry that year. Film wh...        51     25  2023-01-18 11:25:19\n",
       "3         4     danielsmary  Laugh explain situation career occur serious. ...        37     18  2023-04-10 22:06:29\n",
       "4         5      carlwarren  Involve sense former often approach government...        27     80  2023-01-24 07:12:21"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veriler.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e9fbe1eb-e3e0-4aa9-a443-20eb46bbf772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet_ID     0\n",
       "Username     0\n",
       "Text         0\n",
       "Retweets     0\n",
       "Likes        0\n",
       "Timestamp    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veriler.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "222fddfe-ebec-4120-bf8c-e0f4bb13ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metin_list = veriler[\"Text\"].to_list()\n",
    "metin_list_ham_veri = metin_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65b477-124b-4b14-8d1f-232447d7979e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metin_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af9509-b1c2-45d5-a0fa-cea2b6137a2b",
   "metadata": {},
   "source": [
    "    Adım adım ön işleme\n",
    "- Lowercase\n",
    "- Remove Punctuation\n",
    "- Handling Hashtags, Mentions, URLs\n",
    "- Remove Irrelevant Emojis\n",
    "- Frequency-based Word Removal\n",
    "- Remove Stopwords\n",
    "- Lexicon Normalization (Lemmatization or Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002767c-e4f5-448a-8d71-6b403a6a533b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## lowercase \n",
    "metin_list = [tweet.lower() for tweet in metin_list]\n",
    "## noktalama işaretleri\n",
    "metin_list = [''.join([char for char in tweet if char not in string.punctuation]) for tweet in metin_list]\n",
    "## stopwords kelimeleri \n",
    "stop_words = set(stopwords.words('english'))  # veya Türkçe için 'turkish'\n",
    "metin_list = [' '.join([word for word in tweet.split() if word not in stop_words]) for tweet in metin_list]\n",
    "## mentions http olan içerikler\n",
    "metin_list = [re.sub(r'@\\w+|https?://\\S+|#', '', tweet) for tweet in metin_list]\n",
    "## emojiler\n",
    "metin_list = [re.sub(r'[^\\x00-\\x7F]+', '', tweet) for tweet in metin_list]\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"Ham veri : \", metin_list_ham_veri[:10])\n",
    "print(\"*\"*100)\n",
    "print(\"İşlenmiş veri : \",metin_list[:10])\n",
    "print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "45fe067b-99f9-447e-ac1e-78f3bdada97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens :  ['party', 'least', 'receive', 'say', 'single', 'prevent', 'prevent', 'husband', 'affect', 'may']\n"
     ]
    }
   ],
   "source": [
    "## frekans\n",
    "kelimeler = nltk.word_tokenize(\"\".join(metin_list))\n",
    "print(\"Tokens : \",kelimeler[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b568bc-3e81-44d2-9137-9373da464f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frekans = Counter(kelimeler)\n",
    "frekans\n",
    "\n",
    "## kelime sıklıkları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e71141-76f1-42dc-b173-5d093fd65ec0",
   "metadata": {},
   "source": [
    "### Lemmatization veya Stemming işlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ed1f6c0d-e57f-420d-a7ea-9f6a24429107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelimelerin POS etiketlerini al\n",
    "pos_kelimeler = pos_tag(kelimeler)\n",
    "\n",
    "# Lemmatizer'ı oluştur\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Kelimeleri lemmatize et\n",
    "lemma_kelimeler = [\n",
    "    lemmatizer.lemmatize(kelime, pos=pos[0].lower()) if pos[0].lower() in ['a', 'r', 'n', 'v'] else lemmatizer.lemmatize(kelime)\n",
    "    for kelime, pos in pos_kelimeler\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "57f62406-81d4-449a-a41b-42dddf38fd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before :  ['party', 'least', 'receive', 'say', 'single', 'prevent', 'prevent', 'husband', 'affect', 'may', 'cup', 'style', 'evening', 'protect', 'effect', 'another', 'stage', 'perform', 'possible', 'try', 'tax', 'share', 'style', 'television', 'successful', 'much', 'sell', 'development', 'economy', 'effecthotel', 'still', 'congress', 'may', 'member', 'staff', 'media', 'draw', 'buy', 'fly', 'identify', 'another', 'turn', 'minute', 'would', 'local', 'subject', 'way', 'believe', 'question', 'message', 'imagine', 'join', 'agency', 'indicatenice', 'debate', 'industry', 'year', 'film', 'generation', 'push', 'discover', 'partner', 'level', 'nearly', 'money', 'store', 'style', 'may', 'enjoy', 'kid', 'discuss', 'blue', 'save', 'model', 'another', 'along', 'everybody', 'especially', 'dinner', 'character', 'yardlaugh', 'explain', 'situation', 'career', 'occur', 'serious', 'five', 'particular', 'important', 'size']\n",
      "****************************************************************************************************\n",
      "After :  ['party', 'least', 'receive', 'say', 'single', 'prevent', 'prevent', 'husband', 'affect', 'may', 'cup', 'style', 'even', 'protect', 'effect', 'another', 'stage', 'perform', 'possible', 'try', 'tax', 'share', 'style', 'television', 'successful', 'much', 'sell', 'development', 'economy', 'effecthotel', 'still', 'congress', 'may', 'member', 'staff', 'medium', 'draw', 'buy', 'fly', 'identify', 'another', 'turn', 'minute', 'would', 'local', 'subject', 'way', 'believe', 'question', 'message', 'imagine', 'join', 'agency', 'indicatenice', 'debate', 'industry', 'year', 'film', 'generation', 'push', 'discover', 'partner', 'level', 'nearly', 'money', 'store', 'style', 'may', 'enjoy', 'kid', 'discus', 'blue', 'save', 'model', 'another', 'along', 'everybody', 'especially', 'dinner', 'character', 'yardlaugh', 'explain', 'situation', 'career', 'occur', 'serious', 'five', 'particular', 'important', 'size']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before : \", kelimeler[:90])\n",
    "print(\"*\" * 100)\n",
    "print(\"After : \", lemma_kelimeler[:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "87c614a8-005c-48fd-91d6-d4a9e1181cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(metin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9c6e0069-95bc-48cf-8108-bc29aa0ddce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal Kelime Matrisi (Var/Yok)\n",
      "   build  building  business  buy  call  camera  campaign  candidate  capital  car\n",
      "0      0         0         0    0     0       0         0          0        0    0\n",
      "1      0         0         0    1     0       0         0          0        0    0\n",
      "2      0         0         0    0     0       0         0          0        0    0\n",
      "3      0         0         0    0     0       0         0          1        0    0\n",
      "4      0         0         0    0     0       0         0          0        0    0\n",
      "5      0         0         0    0     0       0         0          0        0    0\n",
      "6      1         0         0    0     0       0         0          0        0    0\n",
      "7      0         0         0    0     0       0         0          0        0    0\n",
      "8      0         0         0    0     0       0         0          0        0    0\n",
      "9      0         0         0    0     0       0         0          0        0    0\n",
      "\n",
      "Lemmatization Sonrası Kelime Matrisi (Var/Yok)\n",
      "   build  building  business  buy  call  camera  campaign  candidate  capital  car\n",
      "0      0         0         0    0     0       0         0          0        0    0\n",
      "1      0         0         0    1     0       0         0          0        0    0\n",
      "2      0         0         0    0     0       0         0          0        0    0\n",
      "3      0         0         0    0     0       0         0          1        0    0\n",
      "4      0         0         0    0     0       0         0          0        0    0\n",
      "5      0         0         0    0     0       0         0          0        0    0\n",
      "6      1         0         0    0     0       0         0          0        0    0\n",
      "7      0         0         0    0     0       0         0          0        0    0\n",
      "8      0         0         0    0     0       0         0          0        0    0\n",
      "9      0         0         0    0     0       0         0          0        0    0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Çıktıyı yan yana görebilmek adına\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 250)\n",
    "# Kelimeleri sütun isimleri olarak alıyoruz\n",
    "df_original = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Orijinal Kelime Matrisi (Var/Yok)\")\n",
    "print(df_original.iloc[:10,100:110])\n",
    "\n",
    "# Lemmatization işlemi için WordNetLemmatizer kullanıyoruz\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Her kelimenin türünü bulup lemmatize ediyoruz\n",
    "words_pos = pos_tag(vectorizer.get_feature_names_out())\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in words_pos]\n",
    "\n",
    "# Lemmatized kelimelerle bir dict oluşturup aynı kelimeyi tekrar etmemesi için set kullanıyoruz\n",
    "lemmatized_word_dict = {word: lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in words_pos}\n",
    "df_lemmatized = df_original.rename(columns=lemmatized_word_dict)\n",
    "\n",
    "# Aynı köke sahip olan kelimeleri birleştiriyoruz\n",
    "df_lemmatized = df_lemmatized.groupby(axis=1, level=0, sort=False).sum()\n",
    "\n",
    "print(\"\\nLemmatization Sonrası Kelime Matrisi (Var/Yok)\")\n",
    "print(df_lemmatized.iloc[:10,100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "984eb75b-8389-43ba-8519-f94cf6d1e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>islenmis_icerik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>julie81</td>\n",
       "      <td>Party least receive say or single. Prevent pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-30 11:00:51</td>\n",
       "      <td>party least receive say single prevent prevent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>richardhester</td>\n",
       "      <td>Hotel still Congress may member staff. Media d...</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-02 22:45:58</td>\n",
       "      <td>hotel still congress may member staff media dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>williamsjoseph</td>\n",
       "      <td>Nice be her debate industry that year. Film wh...</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-18 11:25:19</td>\n",
       "      <td>nice debate industry year film generation push...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>danielsmary</td>\n",
       "      <td>Laugh explain situation career occur serious. ...</td>\n",
       "      <td>37</td>\n",
       "      <td>18</td>\n",
       "      <td>2023-04-10 22:06:29</td>\n",
       "      <td>laugh explain situation career occur serious f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>carlwarren</td>\n",
       "      <td>Involve sense former often approach government...</td>\n",
       "      <td>27</td>\n",
       "      <td>80</td>\n",
       "      <td>2023-01-24 07:12:21</td>\n",
       "      <td>involve sense former often approach government...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID        Username                                               Text  Retweets  Likes            Timestamp                                    islenmis_icerik\n",
       "0         1         julie81  Party least receive say or single. Prevent pre...         2     25  2023-01-30 11:00:51  party least receive say single prevent prevent...\n",
       "1         2   richardhester  Hotel still Congress may member staff. Media d...        35     29  2023-01-02 22:45:58  hotel still congress may member staff media dr...\n",
       "2         3  williamsjoseph  Nice be her debate industry that year. Film wh...        51     25  2023-01-18 11:25:19  nice debate industry year film generation push...\n",
       "3         4     danielsmary  Laugh explain situation career occur serious. ...        37     18  2023-04-10 22:06:29  laugh explain situation career occur serious f...\n",
       "4         5      carlwarren  Involve sense former often approach government...        27     80  2023-01-24 07:12:21  involve sense former often approach government..."
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veriler.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a36897-1b74-48c6-8f78-dd52b09e8856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metin_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1805ad-63ed-4a84-bdf6-7b7ac8bafe86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(frekans))\n",
    "print(frekans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2e6b2ade-5a61-46c4-9ebd-74cc6300eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "least = frekans.values()\n",
    "least_key = frekans.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0154174-3239-437b-8252-2e17a40b9720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(least)\n",
    "print(least_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bd9d0076-d3d7-4294-99c2-b8f500d46332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En az tekrar eden kelimeler ve sıklıkları :  [('neverbody', 1), ('lineguess', 1), ('kindday', 1), ('seemborn', 1), ('manyagree', 1), ('becomeboy', 1), ('safemajor', 1), ('militarylisten', 1), ('troubledoctor', 1), ('viewrepresent', 1)]\n",
      "****************************************************************************************************\n",
      "En fazla tekrar eden kelimeler ve sıklıkları :  [('hard', 393), ('tax', 382), ('maybe', 377), ('job', 376), ('wear', 371), ('instead', 370), ('add', 370), ('piece', 369), ('check', 369), ('young', 369)]\n"
     ]
    }
   ],
   "source": [
    "def least_words(counter, n=10):\n",
    "    # En az tekrar eden n kelimeyi al\n",
    "    least = counter.most_common()[:-n-1:-1]  # En az tekrar eden n kelime\n",
    "    least_keys, least_values = zip(*least)  # Anahtar ve değerleri ayır\n",
    "    a = list(zip(least_keys, least_values))  # Anahtar ve değerleri birleştir ve listeye dönüştür\n",
    "    # Sonuçları döndür\n",
    "    return a\n",
    "\n",
    "def most_words(counter,n=10):\n",
    "    most = counter.most_common(10)\n",
    "    least_keys, least_values = zip(*most)  # Anahtar ve değerleri ayır\n",
    "    a = list(zip(least_keys, least_values))  # Anahtar ve değerleri birleştir ve listeye dönüştür\n",
    "    return a\n",
    "\n",
    "\n",
    "print(\"En az tekrar eden kelimeler ve sıklıkları : \",least_words(frekans))\n",
    "print(\"*\"*100)\n",
    "print(\"En fazla tekrar eden kelimeler ve sıklıkları : \",most_words(frekans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab0b98-dbdc-4fb0-b1c8-313e19656ad8",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097aab4-33cb-415c-85cc-a89110556dd3",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2bed3-b55f-4a15-9eca-61cbbd416a84",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea4cc2-c4f7-4c7c-a7ca-ec6a87cddfef",
   "metadata": {},
   "source": [
    "## Gender Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c20d61-780c-4fb6-837c-66eee7a5c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4602a1c0-aca6-42c6-ac60-9aed8f8e7c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>main; @Kan1shk3</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>ÛÏIt felt like they were my friends and I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>clcncl</td>\n",
       "      <td>Belgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  815719226    False   finalized                   3    10/26/15 23:24   \n",
       "1  815719227    False   finalized                   3    10/26/15 23:30   \n",
       "2  815719228    False   finalized                   3    10/26/15 23:33   \n",
       "3  815719229    False   finalized                   3    10/26/15 23:10   \n",
       "4  815719230    False   finalized                   3     10/27/15 1:15   \n",
       "\n",
       "   gender  gender:confidence profile_yn  profile_yn:confidence  \\\n",
       "0    male             1.0000        yes                    1.0   \n",
       "1    male             1.0000        yes                    1.0   \n",
       "2    male             0.6625        yes                    1.0   \n",
       "3    male             1.0000        yes                    1.0   \n",
       "4  female             1.0000        yes                    1.0   \n",
       "\n",
       "          created  ...                                       profileimage  \\\n",
       "0    12/5/13 1:48  ...  https://pbs.twimg.com/profile_images/414342229...   \n",
       "1   10/1/12 13:51  ...  https://pbs.twimg.com/profile_images/539604221...   \n",
       "2  11/28/14 11:30  ...  https://pbs.twimg.com/profile_images/657330418...   \n",
       "3   6/11/09 22:39  ...  https://pbs.twimg.com/profile_images/259703936...   \n",
       "4   4/16/14 13:23  ...  https://pbs.twimg.com/profile_images/564094871...   \n",
       "\n",
       "   retweet_count sidebar_color  \\\n",
       "0              0        FFFFFF   \n",
       "1              0        C0DEED   \n",
       "2              1        C0DEED   \n",
       "3              0        C0DEED   \n",
       "4              0             0   \n",
       "\n",
       "                                                text tweet_coord tweet_count  \\\n",
       "0  Robbie E Responds To Critics After Win Against...         NaN      110964   \n",
       "1  ÛÏIt felt like they were my friends and I was...         NaN        7471   \n",
       "2  i absolutely adore when louis starts the songs...         NaN        5617   \n",
       "3  Hi @JordanSpieth - Looking at the url - do you...         NaN        1693   \n",
       "4  Watching Neighbours on Sky+ catching up with t...         NaN       31462   \n",
       "\n",
       "    tweet_created      tweet_id   tweet_location               user_timezone  \n",
       "0  10/26/15 12:40  6.587300e+17  main; @Kan1shk3                     Chennai  \n",
       "1  10/26/15 12:40  6.587300e+17              NaN  Eastern Time (US & Canada)  \n",
       "2  10/26/15 12:40  6.587300e+17           clcncl                    Belgrade  \n",
       "3  10/26/15 12:40  6.587300e+17    Palo Alto, CA  Pacific Time (US & Canada)  \n",
       "4  10/26/15 12:40  6.587300e+17              NaN                         NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"gender_classifier.csv\",encoding=\"latin1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d198c6a-86af-4587-b097-0e5bdcf47426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20050, 26)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7418aa0b-edd0-4b8c-98cc-551f7161d761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>you don't know me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>A global marketplace for images, videos and mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>male</td>\n",
       "      <td>The secret of getting ahead is getting started.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female</td>\n",
       "      <td>Pll Fan // Crazy about MCD // Ramen is bae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>Renaissance art historian, University of Notti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender                                        description\n",
       "0    male                              i sing my own rhythm.\n",
       "1    male  I'm the author of novels filled with family dr...\n",
       "2    male                louis whining and squealing and all\n",
       "3    male  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...\n",
       "4  female  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...\n",
       "5  female                                 you don't know me.\n",
       "6   brand  A global marketplace for images, videos and mu...\n",
       "7    male    The secret of getting ahead is getting started.\n",
       "8  female         Pll Fan // Crazy about MCD // Ramen is bae\n",
       "9  female  Renaissance art historian, University of Notti..."
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.concat([data.gender,data.description],axis=1)\n",
    "data1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "87a06fd7-1b9d-420f-95de-f82aabbb4dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20050, 2)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ed83f9e0-579c-4014-8c14-52df85162cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16224, 2)\n"
     ]
    }
   ],
   "source": [
    "data1.dropna(inplace=True)\n",
    "data1.reset_index(drop=True,inplace=True)\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1ed47dd4-e74d-4fe3-a54e-551598fb4fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender                                        description\n",
       "0       0                              i sing my own rhythm.\n",
       "1       0  I'm the author of novels filled with family dr...\n",
       "2       0                louis whining and squealing and all\n",
       "3       0  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...\n",
       "4       1  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T..."
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.gender = [1 if i == \"female\" else 0 for i in data1.gender]\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d09300f0-1357-4ecb-9293-18f418c16774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d2788f25-dddf-4a16-9eb5-274a85617fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7b81ae1f-d4d7-402a-926b-c28dff754ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = []\n",
    "\n",
    "for i in range(1000):\n",
    "    metin = re.sub(\"[^a-zA-Z]\",\" \",data1.description[i])\n",
    "    metin = metin.lower()\n",
    "    metin = metin.split()\n",
    "    metin = [lemma.lemmatize(j) for j in metin if not j in set(stopwords.words(\"english\"))]\n",
    "    metinson = \" \".join(metin)\n",
    "    liste.append(metinson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "506f0725-85c1-4f5d-a088-3e7571316c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10000)\n",
    "x = cv.fit_transform(liste).toarray()\n",
    "y = data1.iloc[:1000,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9aa004d4-8058-4e21-9253-5807b9aa18c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sing rhythm',\n",
       " 'author novel filled family drama romance',\n",
       " 'louis whining squealing',\n",
       " 'mobile guy er shazam google kleiner perkins yahoo sprint pc airtouch air force stanford gsb uva dad husband brother golfer',\n",
       " 'ricky wilson best frontman kaiser chief best band xxxx thank kaiser chief incredible year gig memory cherish always xxxxxxx',\n",
       " 'know',\n",
       " 'global marketplace image video music sharing photo inspiration design tip video creative community',\n",
       " 'secret getting ahead getting started',\n",
       " 'pll fan crazy mcd ramen bae',\n",
       " 'renaissance art historian university nottingham fuelled haribo partial coffee soft spot renaissance china national teaching fellow']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d6bdf55f-64d8-4e23-8ca2-c259e8617e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bdc5d436-1db9-4fd1-a4fe-b7c6f9669961",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "21a3cf65-2af0-4aa2-bee0-ccdea42660e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gn = GaussianNB()\n",
    "gn.fit(x_train,y_train)\n",
    "yhead = gn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9997d2e0-e5a7-4597-af8e-8c7e96b7381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96 46]\n",
      " [32 26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,yhead)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8cff7b7e-b330-4498-bfae-1f5f61a2a60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(yhead, y_test))  # Compare predictions with true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ab9c3988-0104-47e5-95a1-9cf5052d7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5ee46697-2c85-473e-a4fb-7344d40daea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy score : 0.6933333333333334\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "XGBoost\n",
      "Accuracy score : 0.6733333333333333\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Accuracy score : 0.54\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Random Forest\n",
      "Accuracy score : 0.6933333333333334\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(model_name)\n",
    "    print(f\"Accuracy score : {accuracy_score(y_pred, y_test)}\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1b342-2705-455b-a4e4-7bdbdf148100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
